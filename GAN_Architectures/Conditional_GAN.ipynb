{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24ab7562",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 128\n",
    "z_dim = 100\n",
    "num_classes = 10\n",
    "img_size = 28\n",
    "channels = 1\n",
    "epochs = 50\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "os.makedirs(\"cgan_generated\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bc38512",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('.', train=True, download=True, transform=transform),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef929bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim , num_classes , img_shape):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes , num_classes)\n",
    "        self.img_shape = img_shape\n",
    "        input_dim = z_dim + num_classes\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim , 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256,512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512,1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.ReLU(True),\n",
    "\n",
    "            nn.Linear(1024, int(torch.prod(torch.tensor(img_shape)))),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "    \n",
    "    def forward(self , noise , labels):\n",
    "        # Concatenate noise and label embeddings\n",
    "        x = torch.cat([noise, self.label_emb(labels)], dim =1)\n",
    "        img = self.model(x)\n",
    "        img = img.view(x.size(0), *self.img_shape)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d39cd8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_classes, img_shape):\n",
    "        super().__init__()\n",
    "        self.label_emb = nn.Embedding(num_classes, num_classes)\n",
    "        input_dim = int(torch.prod(torch.tensor(img_shape))) + num_classes\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        # Flatten image and concatenate label\n",
    "        img_flat = img.view(img.size(0), -1)\n",
    "        x = torch.cat([img_flat, self.label_emb(labels)], dim=1)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1612d1e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_shape = (channels,img_size,img_size)\n",
    "generator = Generator(z_dim, num_classes, img_shape).to(device)\n",
    "discriminator = Discriminator(num_classes, img_shape).to(device)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=lr,betas = (beta1,0.999))\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr = lr , betas = (beta1,0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "220f31e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "k = 3   \n",
    "p = 1  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e71d7b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/50] [Batch 0/469] D Loss: 1.3803 | G Loss: 0.5997\n",
      "[Epoch 1/50] [Batch 200/469] D Loss: 1.4013 | G Loss: 0.6500\n",
      "[Epoch 1/50] [Batch 400/469] D Loss: 1.3944 | G Loss: 0.6637\n",
      "[Epoch 2/50] [Batch 0/469] D Loss: 1.4035 | G Loss: 0.6351\n",
      "[Epoch 2/50] [Batch 200/469] D Loss: 1.3562 | G Loss: 0.6862\n",
      "[Epoch 2/50] [Batch 400/469] D Loss: 1.3861 | G Loss: 0.6827\n",
      "[Epoch 3/50] [Batch 0/469] D Loss: 1.3881 | G Loss: 0.7172\n",
      "[Epoch 3/50] [Batch 200/469] D Loss: 1.4115 | G Loss: 0.7018\n",
      "[Epoch 3/50] [Batch 400/469] D Loss: 1.3792 | G Loss: 0.6998\n",
      "[Epoch 4/50] [Batch 0/469] D Loss: 1.3824 | G Loss: 0.6938\n",
      "[Epoch 4/50] [Batch 200/469] D Loss: 1.3789 | G Loss: 0.7123\n",
      "[Epoch 4/50] [Batch 400/469] D Loss: 1.4229 | G Loss: 0.6721\n",
      "[Epoch 5/50] [Batch 0/469] D Loss: 1.3801 | G Loss: 0.7074\n",
      "[Epoch 5/50] [Batch 200/469] D Loss: 1.4559 | G Loss: 0.6966\n",
      "[Epoch 5/50] [Batch 400/469] D Loss: 1.3939 | G Loss: 0.7137\n",
      "[Epoch 6/50] [Batch 0/469] D Loss: 1.3979 | G Loss: 0.7034\n",
      "[Epoch 6/50] [Batch 200/469] D Loss: 1.3964 | G Loss: 0.7186\n",
      "[Epoch 6/50] [Batch 400/469] D Loss: 1.3830 | G Loss: 0.6985\n",
      "[Epoch 7/50] [Batch 0/469] D Loss: 1.3816 | G Loss: 0.7400\n",
      "[Epoch 7/50] [Batch 200/469] D Loss: 1.3919 | G Loss: 0.7194\n",
      "[Epoch 7/50] [Batch 400/469] D Loss: 1.3717 | G Loss: 0.7115\n",
      "[Epoch 8/50] [Batch 0/469] D Loss: 1.3762 | G Loss: 0.6988\n",
      "[Epoch 8/50] [Batch 200/469] D Loss: 1.3995 | G Loss: 0.6914\n",
      "[Epoch 8/50] [Batch 400/469] D Loss: 1.3749 | G Loss: 0.6952\n",
      "[Epoch 9/50] [Batch 0/469] D Loss: 1.3112 | G Loss: 0.7329\n",
      "[Epoch 9/50] [Batch 200/469] D Loss: 1.3607 | G Loss: 0.7625\n",
      "[Epoch 9/50] [Batch 400/469] D Loss: 1.3303 | G Loss: 0.7145\n",
      "[Epoch 10/50] [Batch 0/469] D Loss: 1.3519 | G Loss: 0.7292\n",
      "[Epoch 10/50] [Batch 200/469] D Loss: 1.3614 | G Loss: 0.7323\n",
      "[Epoch 10/50] [Batch 400/469] D Loss: 1.3220 | G Loss: 0.7770\n",
      "[Epoch 11/50] [Batch 0/469] D Loss: 1.3755 | G Loss: 0.6998\n",
      "[Epoch 11/50] [Batch 200/469] D Loss: 1.3792 | G Loss: 0.7063\n",
      "[Epoch 11/50] [Batch 400/469] D Loss: 1.3703 | G Loss: 0.7207\n",
      "[Epoch 12/50] [Batch 0/469] D Loss: 1.4193 | G Loss: 0.7164\n",
      "[Epoch 12/50] [Batch 200/469] D Loss: 1.3919 | G Loss: 0.6986\n",
      "[Epoch 12/50] [Batch 400/469] D Loss: 1.3447 | G Loss: 0.7230\n",
      "[Epoch 13/50] [Batch 0/469] D Loss: 1.3814 | G Loss: 0.7199\n",
      "[Epoch 13/50] [Batch 200/469] D Loss: 1.3895 | G Loss: 0.7539\n",
      "[Epoch 13/50] [Batch 400/469] D Loss: 1.3211 | G Loss: 0.7144\n",
      "[Epoch 14/50] [Batch 0/469] D Loss: 1.3044 | G Loss: 0.7443\n",
      "[Epoch 14/50] [Batch 200/469] D Loss: 1.3989 | G Loss: 0.7021\n",
      "[Epoch 14/50] [Batch 400/469] D Loss: 1.3982 | G Loss: 0.6687\n",
      "[Epoch 15/50] [Batch 0/469] D Loss: 1.3029 | G Loss: 0.8241\n",
      "[Epoch 15/50] [Batch 200/469] D Loss: 1.3344 | G Loss: 0.7044\n",
      "[Epoch 15/50] [Batch 400/469] D Loss: 1.3543 | G Loss: 0.9039\n",
      "[Epoch 16/50] [Batch 0/469] D Loss: 1.2959 | G Loss: 0.8429\n",
      "[Epoch 16/50] [Batch 200/469] D Loss: 1.2972 | G Loss: 0.7645\n",
      "[Epoch 16/50] [Batch 400/469] D Loss: 1.3250 | G Loss: 0.8679\n",
      "[Epoch 17/50] [Batch 0/469] D Loss: 1.2895 | G Loss: 0.8432\n",
      "[Epoch 17/50] [Batch 200/469] D Loss: 1.3519 | G Loss: 0.7441\n",
      "[Epoch 17/50] [Batch 400/469] D Loss: 1.3046 | G Loss: 0.8697\n",
      "[Epoch 18/50] [Batch 0/469] D Loss: 1.2452 | G Loss: 0.8070\n",
      "[Epoch 18/50] [Batch 200/469] D Loss: 1.2296 | G Loss: 0.8455\n",
      "[Epoch 18/50] [Batch 400/469] D Loss: 1.2535 | G Loss: 1.0118\n",
      "[Epoch 19/50] [Batch 0/469] D Loss: 1.3465 | G Loss: 0.8365\n",
      "[Epoch 19/50] [Batch 200/469] D Loss: 1.2519 | G Loss: 1.1068\n",
      "[Epoch 19/50] [Batch 400/469] D Loss: 1.2555 | G Loss: 0.8933\n",
      "[Epoch 20/50] [Batch 0/469] D Loss: 1.3354 | G Loss: 0.9289\n",
      "[Epoch 20/50] [Batch 200/469] D Loss: 1.3124 | G Loss: 0.7886\n",
      "[Epoch 20/50] [Batch 400/469] D Loss: 1.2360 | G Loss: 1.0019\n",
      "[Epoch 21/50] [Batch 0/469] D Loss: 1.2563 | G Loss: 0.7622\n",
      "[Epoch 21/50] [Batch 200/469] D Loss: 1.3185 | G Loss: 1.0885\n",
      "[Epoch 21/50] [Batch 400/469] D Loss: 1.1621 | G Loss: 1.0630\n",
      "[Epoch 22/50] [Batch 0/469] D Loss: 1.3261 | G Loss: 0.8382\n",
      "[Epoch 22/50] [Batch 200/469] D Loss: 1.2335 | G Loss: 0.8074\n",
      "[Epoch 22/50] [Batch 400/469] D Loss: 1.2453 | G Loss: 0.8478\n",
      "[Epoch 23/50] [Batch 0/469] D Loss: 1.2504 | G Loss: 0.8757\n",
      "[Epoch 23/50] [Batch 200/469] D Loss: 1.3060 | G Loss: 0.8022\n",
      "[Epoch 23/50] [Batch 400/469] D Loss: 1.3109 | G Loss: 1.0321\n",
      "[Epoch 24/50] [Batch 0/469] D Loss: 1.2292 | G Loss: 0.8977\n",
      "[Epoch 24/50] [Batch 200/469] D Loss: 1.2848 | G Loss: 0.9695\n",
      "[Epoch 24/50] [Batch 400/469] D Loss: 1.2358 | G Loss: 0.9383\n",
      "[Epoch 25/50] [Batch 0/469] D Loss: 1.2415 | G Loss: 0.9771\n",
      "[Epoch 25/50] [Batch 200/469] D Loss: 1.3013 | G Loss: 1.0001\n",
      "[Epoch 25/50] [Batch 400/469] D Loss: 1.3378 | G Loss: 0.8209\n",
      "[Epoch 26/50] [Batch 0/469] D Loss: 1.1637 | G Loss: 0.9011\n",
      "[Epoch 26/50] [Batch 200/469] D Loss: 1.2025 | G Loss: 0.9871\n",
      "[Epoch 26/50] [Batch 400/469] D Loss: 1.3219 | G Loss: 0.8604\n",
      "[Epoch 27/50] [Batch 0/469] D Loss: 1.2736 | G Loss: 0.8195\n",
      "[Epoch 27/50] [Batch 200/469] D Loss: 1.2685 | G Loss: 1.0339\n",
      "[Epoch 27/50] [Batch 400/469] D Loss: 1.2570 | G Loss: 0.9043\n",
      "[Epoch 28/50] [Batch 0/469] D Loss: 1.2295 | G Loss: 0.9809\n",
      "[Epoch 28/50] [Batch 200/469] D Loss: 1.3140 | G Loss: 1.1298\n",
      "[Epoch 28/50] [Batch 400/469] D Loss: 1.2518 | G Loss: 0.9289\n",
      "[Epoch 29/50] [Batch 0/469] D Loss: 1.2536 | G Loss: 0.9772\n",
      "[Epoch 29/50] [Batch 200/469] D Loss: 1.2206 | G Loss: 1.0254\n",
      "[Epoch 29/50] [Batch 400/469] D Loss: 1.2355 | G Loss: 1.0007\n",
      "[Epoch 30/50] [Batch 0/469] D Loss: 1.2773 | G Loss: 0.9466\n",
      "[Epoch 30/50] [Batch 200/469] D Loss: 1.2545 | G Loss: 0.9479\n",
      "[Epoch 30/50] [Batch 400/469] D Loss: 1.2247 | G Loss: 1.0663\n",
      "[Epoch 31/50] [Batch 0/469] D Loss: 1.3403 | G Loss: 1.0468\n",
      "[Epoch 31/50] [Batch 200/469] D Loss: 1.1978 | G Loss: 1.0906\n",
      "[Epoch 31/50] [Batch 400/469] D Loss: 1.2420 | G Loss: 1.1247\n",
      "[Epoch 32/50] [Batch 0/469] D Loss: 1.1653 | G Loss: 1.0023\n",
      "[Epoch 32/50] [Batch 200/469] D Loss: 1.1406 | G Loss: 1.0914\n",
      "[Epoch 32/50] [Batch 400/469] D Loss: 1.2600 | G Loss: 1.0174\n",
      "[Epoch 33/50] [Batch 0/469] D Loss: 1.1826 | G Loss: 1.0268\n",
      "[Epoch 33/50] [Batch 200/469] D Loss: 1.2781 | G Loss: 0.9460\n",
      "[Epoch 33/50] [Batch 400/469] D Loss: 1.2593 | G Loss: 1.1470\n",
      "[Epoch 34/50] [Batch 0/469] D Loss: 1.2452 | G Loss: 1.0746\n",
      "[Epoch 34/50] [Batch 200/469] D Loss: 1.2495 | G Loss: 1.0663\n",
      "[Epoch 34/50] [Batch 400/469] D Loss: 1.2617 | G Loss: 1.2724\n",
      "[Epoch 35/50] [Batch 0/469] D Loss: 1.2892 | G Loss: 1.2375\n",
      "[Epoch 35/50] [Batch 200/469] D Loss: 1.3745 | G Loss: 0.8561\n",
      "[Epoch 35/50] [Batch 400/469] D Loss: 1.2952 | G Loss: 0.8950\n",
      "[Epoch 36/50] [Batch 0/469] D Loss: 1.3004 | G Loss: 1.0017\n",
      "[Epoch 36/50] [Batch 200/469] D Loss: 1.3270 | G Loss: 1.0916\n",
      "[Epoch 36/50] [Batch 400/469] D Loss: 1.2239 | G Loss: 1.0620\n",
      "[Epoch 37/50] [Batch 0/469] D Loss: 1.2437 | G Loss: 1.0307\n",
      "[Epoch 37/50] [Batch 200/469] D Loss: 1.2259 | G Loss: 1.0832\n",
      "[Epoch 37/50] [Batch 400/469] D Loss: 1.2380 | G Loss: 0.9041\n",
      "[Epoch 38/50] [Batch 0/469] D Loss: 1.3488 | G Loss: 1.0807\n",
      "[Epoch 38/50] [Batch 200/469] D Loss: 1.2511 | G Loss: 1.0602\n",
      "[Epoch 38/50] [Batch 400/469] D Loss: 1.2988 | G Loss: 1.2137\n",
      "[Epoch 39/50] [Batch 0/469] D Loss: 1.2092 | G Loss: 0.9797\n",
      "[Epoch 39/50] [Batch 200/469] D Loss: 1.2194 | G Loss: 1.0572\n",
      "[Epoch 39/50] [Batch 400/469] D Loss: 1.1999 | G Loss: 1.2024\n",
      "[Epoch 40/50] [Batch 0/469] D Loss: 1.2384 | G Loss: 1.0943\n",
      "[Epoch 40/50] [Batch 200/469] D Loss: 1.2715 | G Loss: 0.9434\n",
      "[Epoch 40/50] [Batch 400/469] D Loss: 1.2162 | G Loss: 0.9438\n",
      "[Epoch 41/50] [Batch 0/469] D Loss: 1.3163 | G Loss: 0.8742\n",
      "[Epoch 41/50] [Batch 200/469] D Loss: 1.2433 | G Loss: 1.1278\n",
      "[Epoch 41/50] [Batch 400/469] D Loss: 1.2769 | G Loss: 0.9458\n",
      "[Epoch 42/50] [Batch 0/469] D Loss: 1.2546 | G Loss: 1.1270\n",
      "[Epoch 42/50] [Batch 200/469] D Loss: 1.2999 | G Loss: 0.9474\n",
      "[Epoch 42/50] [Batch 400/469] D Loss: 1.2196 | G Loss: 1.0423\n",
      "[Epoch 43/50] [Batch 0/469] D Loss: 1.2295 | G Loss: 0.9599\n",
      "[Epoch 43/50] [Batch 200/469] D Loss: 1.2954 | G Loss: 1.1754\n",
      "[Epoch 43/50] [Batch 400/469] D Loss: 1.2830 | G Loss: 1.0615\n",
      "[Epoch 44/50] [Batch 0/469] D Loss: 1.3061 | G Loss: 0.9332\n",
      "[Epoch 44/50] [Batch 200/469] D Loss: 1.2540 | G Loss: 1.0327\n",
      "[Epoch 44/50] [Batch 400/469] D Loss: 1.3646 | G Loss: 1.0087\n",
      "[Epoch 45/50] [Batch 0/469] D Loss: 1.2128 | G Loss: 1.0164\n",
      "[Epoch 45/50] [Batch 200/469] D Loss: 1.2648 | G Loss: 0.9020\n",
      "[Epoch 45/50] [Batch 400/469] D Loss: 1.2599 | G Loss: 1.0569\n",
      "[Epoch 46/50] [Batch 0/469] D Loss: 1.2967 | G Loss: 0.8026\n",
      "[Epoch 46/50] [Batch 200/469] D Loss: 1.2709 | G Loss: 0.9426\n",
      "[Epoch 46/50] [Batch 400/469] D Loss: 1.3246 | G Loss: 1.0575\n",
      "[Epoch 47/50] [Batch 0/469] D Loss: 1.2726 | G Loss: 1.0723\n",
      "[Epoch 47/50] [Batch 200/469] D Loss: 1.1620 | G Loss: 1.1131\n",
      "[Epoch 47/50] [Batch 400/469] D Loss: 1.3095 | G Loss: 1.0435\n",
      "[Epoch 48/50] [Batch 0/469] D Loss: 1.2740 | G Loss: 1.0689\n",
      "[Epoch 48/50] [Batch 200/469] D Loss: 1.2806 | G Loss: 0.9467\n",
      "[Epoch 48/50] [Batch 400/469] D Loss: 1.2291 | G Loss: 1.0134\n",
      "[Epoch 49/50] [Batch 0/469] D Loss: 1.2738 | G Loss: 1.0352\n",
      "[Epoch 49/50] [Batch 200/469] D Loss: 1.2605 | G Loss: 1.2633\n",
      "[Epoch 49/50] [Batch 400/469] D Loss: 1.3071 | G Loss: 1.1012\n",
      "[Epoch 50/50] [Batch 0/469] D Loss: 1.2722 | G Loss: 1.0994\n",
      "[Epoch 50/50] [Batch 200/469] D Loss: 1.2657 | G Loss: 1.1604\n",
      "[Epoch 50/50] [Batch 400/469] D Loss: 1.2155 | G Loss: 1.0474\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    for i, (real_imgs, real_labels) in enumerate(train_loader):\n",
    "        batch_size_curr = real_imgs.size(0)\n",
    "        real_imgs = real_imgs.to(device)\n",
    "        real_labels = real_labels.to(device)\n",
    "\n",
    "        real = torch.ones(batch_size_curr, 1, device=device)\n",
    "        fake = torch.zeros(batch_size_curr, 1, device=device)\n",
    "\n",
    "        ### ---- Train Discriminator p times ---- ###\n",
    "        for _ in range(p):\n",
    "            z = torch.randn(batch_size_curr, z_dim, device=device)\n",
    "            fake_labels = torch.randint(0, num_classes, (batch_size_curr,), device=device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                gen_imgs = generator(z, fake_labels)\n",
    "\n",
    "            # Real images\n",
    "            real_validity = discriminator(real_imgs, real_labels)\n",
    "            d_real_loss = criterion(real_validity, real)\n",
    "\n",
    "            # Fake images\n",
    "            fake_validity = discriminator(gen_imgs.detach(), fake_labels)\n",
    "            d_fake_loss = criterion(fake_validity, fake)\n",
    "\n",
    "            d_loss = d_real_loss + d_fake_loss\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "        ### ---- Train Generator k times ---- ###\n",
    "        for _ in range(k):\n",
    "            z = torch.randn(batch_size_curr, z_dim, device=device)\n",
    "            gen_labels = torch.randint(0, num_classes, (batch_size_curr,), device=device)\n",
    "            gen_imgs = generator(z, gen_labels)\n",
    "\n",
    "            # Try to fool the discriminator\n",
    "            validity = discriminator(gen_imgs, gen_labels)\n",
    "            g_loss = criterion(validity, real)  # want D(G(z)) = 1\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "        # Print progress\n",
    "        if i % 200 == 0:\n",
    "            print(f\"[Epoch {epoch}/{epochs}] [Batch {i}/{len(train_loader)}] \"\n",
    "                  f\"D Loss: {d_loss.item():.4f} | G Loss: {g_loss.item():.4f}\")\n",
    "\n",
    "    # Save example images after each epoch\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(10,z_dim,device=device)\n",
    "        labels = torch.arange(0, 10, dtype=torch.long, device=device)\n",
    "        samples = generator(z,labels)\n",
    "        samples = samples * 0.5 + 0.5\n",
    "        save_image(samples, f\"cgan_generated/epoch_{epoch}.png\", nrow=10)\n",
    "    generator.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "id": "b91766bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_digit_images(generator, digit, num_samples=16, save_path=None):\n",
    "    generator.eval()\n",
    "    z = torch.randn(num_samples, z_dim).to(device)\n",
    "    labels = torch.full((num_samples,), digit, dtype=torch.long).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_imgs = generator(z, labels)\n",
    "        gen_imgs = gen_imgs * 0.5 + 0.5\n",
    "\n",
    "    if save_path:\n",
    "        save_image(gen_imgs, save_path, nrow=4)\n",
    "        print(f\"Saved to {save_path}\")\n",
    "    return gen_imgs\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "id": "ddd0faad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to cgan_generated/seven.png\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.0000e+00, 0.0000e+00, 1.3709e-06,  ..., 0.0000e+00,\n",
       "           2.9802e-08, 0.0000e+00],\n",
       "          [8.9407e-08, 0.0000e+00, 9.9983e-01,  ..., 2.9802e-08,\n",
       "           2.9802e-08, 5.3346e-06],\n",
       "          [0.0000e+00, 4.3213e-06, 9.9500e-01,  ..., 2.9802e-08,\n",
       "           0.0000e+00, 3.8743e-07],\n",
       "          ...,\n",
       "          [2.9802e-08, 7.9274e-06, 1.9968e-06,  ..., 1.4901e-07,\n",
       "           1.6391e-06, 2.9802e-08],\n",
       "          [1.2517e-06, 5.3644e-05, 9.0361e-05,  ..., 0.0000e+00,\n",
       "           6.2585e-07, 5.9605e-08],\n",
       "          [3.6657e-06, 0.0000e+00, 2.0862e-07,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 4.9829e-05]]],\n",
       "\n",
       "\n",
       "        [[[0.0000e+00, 0.0000e+00, 1.3113e-06,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.9407e-08, 0.0000e+00, 9.9983e-01,  ..., 2.9802e-08,\n",
       "           2.9802e-08, 4.7386e-06],\n",
       "          [0.0000e+00, 4.9472e-06, 9.9578e-01,  ..., 2.9802e-08,\n",
       "           0.0000e+00, 2.9802e-07],\n",
       "          ...,\n",
       "          [2.9802e-08, 9.0003e-06, 2.0862e-06,  ..., 1.7881e-07,\n",
       "           1.6391e-06, 2.9802e-08],\n",
       "          [1.3709e-06, 4.6909e-05, 7.9066e-05,  ..., 0.0000e+00,\n",
       "           5.6624e-07, 5.9605e-08],\n",
       "          [3.6359e-06, 0.0000e+00, 1.7881e-07,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 5.0783e-05]]],\n",
       "\n",
       "\n",
       "        [[[0.0000e+00, 0.0000e+00, 1.3709e-06,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.9407e-08, 0.0000e+00, 9.9983e-01,  ..., 2.9802e-08,\n",
       "           2.9802e-08, 4.8280e-06],\n",
       "          [0.0000e+00, 4.5300e-06, 9.9577e-01,  ..., 2.9802e-08,\n",
       "           0.0000e+00, 3.2783e-07],\n",
       "          ...,\n",
       "          [2.9802e-08, 7.6294e-06, 1.9670e-06,  ..., 1.4901e-07,\n",
       "           1.3709e-06, 2.9802e-08],\n",
       "          [1.1921e-06, 4.7565e-05, 7.8887e-05,  ..., 0.0000e+00,\n",
       "           5.6624e-07, 5.9605e-08],\n",
       "          [3.3081e-06, 0.0000e+00, 1.7881e-07,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 3.9190e-05]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[0.0000e+00, 0.0000e+00, 1.2517e-06,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.9407e-08, 0.0000e+00, 9.9984e-01,  ..., 2.9802e-08,\n",
       "           2.9802e-08, 4.5896e-06],\n",
       "          [0.0000e+00, 4.8578e-06, 9.9653e-01,  ..., 2.9802e-08,\n",
       "           0.0000e+00, 3.2783e-07],\n",
       "          ...,\n",
       "          [0.0000e+00, 7.7486e-06, 2.0266e-06,  ..., 1.4901e-07,\n",
       "           1.5795e-06, 2.9802e-08],\n",
       "          [1.1921e-06, 5.2452e-05, 7.9572e-05,  ..., 0.0000e+00,\n",
       "           4.7684e-07, 2.9802e-08],\n",
       "          [3.2485e-06, 0.0000e+00, 1.7881e-07,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 3.8743e-05]]],\n",
       "\n",
       "\n",
       "        [[[0.0000e+00, 0.0000e+00, 1.4007e-06,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.9407e-08, 0.0000e+00, 9.9980e-01,  ..., 2.9802e-08,\n",
       "           2.9802e-08, 5.0664e-06],\n",
       "          [0.0000e+00, 3.9041e-06, 9.9605e-01,  ..., 2.9802e-08,\n",
       "           0.0000e+00, 3.8743e-07],\n",
       "          ...,\n",
       "          [2.9802e-08, 8.1062e-06, 1.8775e-06,  ..., 1.4901e-07,\n",
       "           1.6987e-06, 2.9802e-08],\n",
       "          [1.2517e-06, 5.0485e-05, 9.5993e-05,  ..., 0.0000e+00,\n",
       "           5.0664e-07, 5.9605e-08],\n",
       "          [3.3081e-06, 0.0000e+00, 1.7881e-07,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 3.8534e-05]]],\n",
       "\n",
       "\n",
       "        [[[0.0000e+00, 0.0000e+00, 1.3113e-06,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 0.0000e+00],\n",
       "          [8.9407e-08, 0.0000e+00, 9.9983e-01,  ..., 2.9802e-08,\n",
       "           2.9802e-08, 5.2750e-06],\n",
       "          [0.0000e+00, 4.9472e-06, 9.9577e-01,  ..., 2.9802e-08,\n",
       "           0.0000e+00, 2.9802e-07],\n",
       "          ...,\n",
       "          [2.9802e-08, 8.9705e-06, 1.9968e-06,  ..., 1.7881e-07,\n",
       "           1.7285e-06, 2.9802e-08],\n",
       "          [1.2517e-06, 4.9919e-05, 7.9393e-05,  ..., 0.0000e+00,\n",
       "           6.2585e-07, 5.9605e-08],\n",
       "          [3.1590e-06, 0.0000e+00, 1.7881e-07,  ..., 0.0000e+00,\n",
       "           0.0000e+00, 4.5180e-05]]]], device='cuda:0')"
      ]
     },
     "execution_count": 529,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_digit_images(generator, digit=7, num_samples=16, save_path=\"cgan_generated/seven.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAN_IITM",
   "language": "python",
   "name": "gan_iitm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
