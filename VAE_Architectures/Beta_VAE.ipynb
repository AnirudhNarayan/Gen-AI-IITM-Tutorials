{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d7b0914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import gc  # Add garbage collector\n",
    "\n",
    "# Clear CUDA memory\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "def print_gpu_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "        print(f\"GPU memory cached: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4960683",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "train_dataset = datasets.MNIST(root = './data' , train = True , download = True , transform = transform)\n",
    "train_loader = DataLoader(train_dataset , batch_size = 64 , shuffle = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3805e7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaVAE(nn.Module):\n",
    "    def __init__(self, latent_dim = 20):\n",
    "        super(BetaVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc_mu = nn.Linear(128 * 4 * 4, latent_dim)\n",
    "        self.fc_logvar = nn.Linear(128 * 4 * 4, latent_dim)\n",
    "\n",
    "        # Decoder\n",
    "        self.fc_decode = nn.Linear(latent_dim, 128 * 4 * 4)\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h_flat = self.flatten(h)\n",
    "        mu = self.fc_mu(h_flat)\n",
    "        logvar = self.fc_logvar(h_flat)\n",
    "        return mu, logvar\n",
    "    \n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        h = self.fc_decode(z)\n",
    "        h = h.view(-1, 128, 4, 4)\n",
    "        x_hat = self.decoder(h)\n",
    "        return x_hat[:, :, :28, :28]\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        x_hat = self.decode(z)\n",
    "        return x_hat, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27b6c57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_vae_loss(x , x_hat ,mu , logvar , beta = 4):\n",
    "    recon_loss = F.binary_cross_entropy(x_hat , x, reduction ='sum')\n",
    "    kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon_loss + beta * kl_div, recon_loss, kl_div\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30609ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 177.78, Recon: 152.42, KL: 6.34\n",
      "Epoch 2, Loss: 152.24, Recon: 116.11, KL: 9.03\n",
      "Epoch 3, Loss: 149.43, Recon: 111.84, KL: 9.40\n",
      "Epoch 4, Loss: 147.94, Recon: 109.79, KL: 9.54\n",
      "Epoch 5, Loss: 146.94, Recon: 108.37, KL: 9.64\n",
      "Epoch 6, Loss: 146.15, Recon: 107.35, KL: 9.70\n",
      "Epoch 7, Loss: 145.72, Recon: 106.60, KL: 9.78\n",
      "Epoch 8, Loss: 145.12, Recon: 105.89, KL: 9.81\n",
      "Epoch 9, Loss: 144.77, Recon: 105.48, KL: 9.82\n",
      "Epoch 10, Loss: 144.46, Recon: 105.05, KL: 9.85\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BetaVAE(latent_dim=20).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "num_epochs = 10\n",
    "beta = 4.0\n",
    "losses, recons, kls = [], [], []\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    model.train()\n",
    "    total_loss, total_recon, total_kl = 0, 0, 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        x, _ = batch\n",
    "        x = x.to(device)\n",
    "        x_hat, mu, logvar = model(x)\n",
    "        loss, recon, kl = beta_vae_loss(x, x_hat, mu, logvar, beta)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_recon += recon.item()\n",
    "        total_kl += kl.item()\n",
    "    \n",
    "    losses.append(total_loss / len(train_loader.dataset))\n",
    "    recons.append(total_recon / len(train_loader.dataset))\n",
    "    kls.append(total_kl / len(train_loader.dataset))\n",
    "\n",
    "    print(f\"Epoch {epoch}, Loss: {losses[-1]:.2f}, Recon: {recons[-1]:.2f}, KL: {kls[-1]:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcac1c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(losses, label='Total Loss')\n",
    "plt.plot(recons, label='Reconstruction Loss')\n",
    "plt.plot(kls, label='KL Divergence')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Beta-VAE Training Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052942ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model , num_images = 16):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = torch.randn(num_images , model.latent_dim).to(device)\n",
    "        samples = model.decode(z)\n",
    "    return samples.cpu()\n",
    "\n",
    "samples = generate_images(model)\n",
    "# Plot generated digits\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i in range(16):\n",
    "    plt.subplot(4, 4, i + 1)\n",
    "    plt.imshow(samples[i][0], cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.suptitle(\"Generated Digits from Beta-VAE\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAN_IITM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
